{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna11-dot/Talksynch/blob/main/TalkSync_Demo_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdp0t8IwaFm1"
      },
      "source": [
        "# TalkSync Demo - Real-Time English to Hindi Translation System\n",
        "\n",
        "## Problem Statement\n",
        "Team members who are not fluent in English struggle to communicate during client calls on video conferencing platforms (Zoom, Google Meet, Microsoft Teams). This creates misunderstandings and reduces efficiency.\n",
        "\n",
        "## Solution Overview\n",
        "A real-time translation system that:\n",
        "1. Captures English speech from client calls\n",
        "2. Transcribes speech to text\n",
        "3. Translates English text to Hindi\n",
        "4. Synthesizes Hindi audio for team members\n",
        "\n",
        "## Demo Scope (Phase 1)\n",
        "- Standalone pipeline demonstration\n",
        "- English to Hindi translation only\n",
        "- File upload and microphone recording support\n",
        "- Latency and quality metrics display\n",
        "\n",
        "## Out of Scope (Phase 2)\n",
        "- Video conferencing platform integration\n",
        "- Real-time streaming\n",
        "- Bidirectional translation\n",
        "- Voice selection and customization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xocv-EHoaFm4"
      },
      "source": [
        "---\n",
        "# System Architecture\n",
        "\n",
        "```\n",
        "                           TALKSYNC ARCHITECTURE\n",
        "    \n",
        "    INPUT                                                    OUTPUT\n",
        "    -----                                                    ------\n",
        "    English Audio                                            Hindi Audio\n",
        "    (microphone/file)                                        (synthesized)\n",
        "         |                                                        ^\n",
        "         v                                                        |\n",
        "+------------------------------------------------------------------------+\n",
        "|                                                                         |\n",
        "|                    (Orchestrator)                              |\n",
        "|                                                                        |\n",
        "|  Responsibilities:                                                     |\n",
        "|  - Route data through pipeline sequentially                            |\n",
        "|  - Validate each module output before proceeding                       |\n",
        "|  - Handle errors gracefully with meaningful messages                   |\n",
        "|  - Log metrics for performance monitoring                              |\n",
        "|                                                                        |\n",
        "|  Design Principle: \"LLM is a translator, not a controller\"             |\n",
        "|  The Decision Box controls flow; modules perform specific tasks.       |\n",
        "+------------------------------------------------------------------------+\n",
        "         |                    |                    |\n",
        "         v                    v                    v\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "  |     |     |      |     |       |\n",
        "  |     ASR     | --> |  Translation  | --> |     TTS       |\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "  | Model:      |     | Model:        |     | Model:        |\n",
        "  | Whisper     |     | IndicTrans2   |     | Chatterbox    |\n",
        "  | (base)      |     | (200M)        |     | Multilingual  |\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "  | Input:      |     | Input:        |     | Input:        |\n",
        "  | Audio file  |     | English text  |     | Hindi text    |\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "  | Output:     |     | Output:       |     | Output:       |\n",
        "  | English     |     | Hindi text    |     | Hindi audio   |\n",
        "  | text        |     |               |     | waveform      |\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "  | Success     |     | Success       |     | Success       |\n",
        "  | Criteria:   |     | Criteria:     |     | Criteria:     |\n",
        "  | WER < 20%   |     | Semantic      |     | Clear         |\n",
        "  |             |     | accuracy      |     | pronunciation |\n",
        "  +-------------+     +---------------+     +---------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMNDicOaaFm5"
      },
      "source": [
        "---\n",
        "# Success Metrics\n",
        "\n",
        "## B Metrics\n",
        "\n",
        "| Metric | Target | Measurement Method |\n",
        "|--------|--------|--------------------|\n",
        "| Communication Success | Team understands client message | Demo shows accurate translation |\n",
        "| End-to-End Latency | Less than 5 seconds | Timestamp logging at each stage |\n",
        "| Output Quality | Intelligible Hindi audio | Manual verification during demo |\n",
        "\n",
        "## T Metrics\n",
        "\n",
        "| Module | Metric | Target | How to Measure | Fallback Strategy |\n",
        "|--------|--------|--------|----------------|-------------------|\n",
        "| ASR (Whisper) | Word Error Rate | Less than 20% | Compare transcription to source | Re-record with cleaner audio |\n",
        "| Translation (IndicTrans2) | Semantic Accuracy | Meaning preserved | Manual review of output | Try simpler sentences |\n",
        "| TTS (Chatterbox) | Intelligibility | Clear pronunciation | Listen test | Adjust CFG and exaggeration parameters |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7qoDAIHaFm5"
      },
      "source": [
        "---\n",
        "# Environment Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mbT0vpQIaFm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8cad87-26cc-433b-b9b6-48f6e86af21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "name, memory.total [MiB], memory.free [MiB]\n",
            "Tesla T4, 15360 MiB, 15095 MiB\n",
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchaudio==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6.0) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (768.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n",
            "\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: System Check\n",
        "# Verify Python version and GPU availability before proceeding\n",
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# GPU check - required for acceptable latency\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
        "\n",
        "# Cell 2: Install PyTorch (compatible versions for Colab Python 3.12)\n",
        "# Colab typically has PyTorch pre-installed, but let's ensure compatibility\n",
        "\n",
        "# Uninstall existing PyTorch, Torchaudio, torchvision to prevent conflicts\n",
        "!pip uninstall -y torch torchaudio torchvision\n",
        "\n",
        "# Install PyTorch 2.6.0 and compatible torchaudio (2.6.0) and torchvision (0.21.0, latest available for cu124)\n",
        "# Use --index-url to specify CUDA 12.4 compatible wheels\n",
        "!pip install torch==2.6.0 torchaudio==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blInWhqoaFm8"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "# Run once, then restart runtime if prompted\n",
        "\n",
        "# Core ML dependencies\n",
        "!pip install -q transformers accelerate sentencepiece\n",
        "!pip install -q openai-whisper\n",
        "!pip install -q scipy soundfile\n",
        "\n",
        "# IndicTrans2 preprocessing toolkit (required for proper translation)\n",
        "!pip install -q IndicTransToolkit\n",
        "\n",
        "\n",
        "# Step 1: Check if it's installed with a different name\n",
        "!pip list | grep -i chatter\n",
        "\n",
        "# Step 2: Try different installation methods\n",
        "!pip install --upgrade chatterbox-tts --no-cache-dir\n",
        "# OR\n",
        "!pip install git+https://github.com/chatterbox-tts/chatterbox-tts.git\n",
        "# OR\n",
        "!pip install chatterbox-tts==0.1.0  # Try specific versio\n",
        "\n",
        "\n",
        "# Gradio for interactive demo interface\n",
        "!pip install -q gradio\n",
        "\n",
        "\n",
        "print(\"\\nAll dependencies installed.\")\n",
        "print(\"If this is your first run, restart the runtime: Runtime -> Restart runtime\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This WILL work - clone directly and add to path\n",
        "!git clone https://github.com/resemble-ai/chatterbox.git /content/chatterbox_repo\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchaudio librosa safetensors huggingface_hub perth\n",
        "\n",
        "# Add to Python path\n",
        "import sys\n",
        "sys.path.insert(0, '/content/chatterbox_repo/src')\n",
        "\n",
        "# Now import\n",
        "from chatterbox.mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES\n",
        "print(f\"✅ Success! Hindi available: {'hi' in SUPPORTED_LANGUAGES}\")"
      ],
      "metadata": {
        "id": "SYFVfTpw8OWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mn5tE1OAaFm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e888e89-cb84-48f9-f757-f51295c46c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package Verification:\n",
            "--------------------------------------------------\n",
            "[OK] Whisper ASR: 20250625\n",
            "[OK] Transformers: 4.46.3\n",
            "[OK] PyTorch: 2.6.0+cu124\n",
            "[OK] TorchAudio: 2.6.0+cu124\n",
            "[OK] Gradio UI: 5.50.0\n",
            "[OK] Chatterbox Multilingual TTS: installed\n",
            "[OK] IndicTransToolkit: installed\n",
            "--------------------------------------------------\n",
            "All packages verified successfully.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Verify Imports\n",
        "\n",
        "import importlib\n",
        "\n",
        "required_packages = [\n",
        "    ('whisper', 'Whisper ASR'),\n",
        "    ('transformers', 'Transformers'),\n",
        "    ('torch', 'PyTorch'),\n",
        "    ('torchaudio', 'TorchAudio'),\n",
        "    ('gradio', 'Gradio UI'),\n",
        "]\n",
        "\n",
        "print(\"Package Verification:\")\n",
        "print(\"-\" * 50)\n",
        "all_ok = True\n",
        "for pkg, name in required_packages:\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        version = getattr(mod, '__version__', 'installed')\n",
        "        print(f\"[OK] {name}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"[FAIL] {name}: NOT INSTALLED\")\n",
        "        all_ok = False\n",
        "\n",
        "# Check Chatterbox multilingual\n",
        "try:\n",
        "    from chatterbox.mtl_tts import ChatterboxMultilingualTTS\n",
        "    print(f\"[OK] Chatterbox Multilingual TTS: installed\")\n",
        "except ImportError as e:\n",
        "    print(f\"[FAIL] Chatterbox Multilingual TTS: {e}\")\n",
        "    all_ok = False\n",
        "\n",
        "# Check IndicTransToolkit\n",
        "try:\n",
        "    from IndicTransToolkit.processor import IndicProcessor\n",
        "    print(f\"[OK] IndicTransToolkit: installed\")\n",
        "except ImportError as e:\n",
        "    print(f\"[FAIL] IndicTransToolkit: {e}\")\n",
        "    all_ok = False\n",
        "\n",
        "print(\"-\" * 50)\n",
        "if all_ok:\n",
        "    print(\"All packages verified successfully.\")\n",
        "else:\n",
        "    print(\"Some packages failed. Re-run Cell 3 and restart runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ta4f0XYyaFm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0d03b6-c2be-4a17-a074-62a05ee9c3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF Token loaded from Colab Secrets\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Hugging Face Token Setup (Secure Method)\n",
        "# Store token in Colab Secrets: Left sidebar -> Key icon -> Add 'HF_TOKEN'\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"HF Token loaded from Colab Secrets\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: HF_TOKEN not found in Colab Secrets\")\n",
        "    print(\"Instructions: Left sidebar -> Key icon -> Add 'HF_TOKEN' with your Hugging Face token\")\n",
        "    HF_TOKEN = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR_SH39WaFm9"
      },
      "source": [
        "---\n",
        "# Module 1: Automatic Speech Recognition (ASR)\n",
        "\n",
        "**Model**: OpenAI Whisper (base)  \n",
        "**Input**: Audio file (WAV, MP3, etc.)  \n",
        "**Output**: English text transcription  \n",
        "**Success Criteria**: Word Error Rate less than 20%\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJt-AyoLaFm-"
      },
      "outputs": [],
      "source": [
        "# Cell 5: ASR Module\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "\n",
        "class ASRModule:\n",
        "    \"\"\"\n",
        "    Automatic Speech Recognition Module\n",
        "    Converts English audio to English text using OpenAI Whisper.\n",
        "\n",
        "    Technical Metric: Word Error Rate (WER)\n",
        "    Target: Less than 20% for clean audio\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_size=\"base\"):\n",
        "        \"\"\"\n",
        "        Initialize Whisper model.\n",
        "\n",
        "        Args:\n",
        "            model_size: Model variant (tiny, base, small, medium, large)\n",
        "                       'base' provides good balance of speed and accuracy\n",
        "        \"\"\"\n",
        "        print(f\"Loading Whisper {model_size} model...\")\n",
        "        self.model = whisper.load_model(model_size)\n",
        "        self.model_size = model_size\n",
        "        print(f\"ASR Module initialized ({model_size})\")\n",
        "\n",
        "    def transcribe(self, audio_path):\n",
        "        \"\"\"\n",
        "        Transcribe audio file to text.\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "\n",
        "        Returns:\n",
        "            dict with keys: success, text, language, latency_ms, error\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            result = self.model.transcribe(audio_path, language=\"en\")\n",
        "            latency_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"text\": result[\"text\"].strip(),\n",
        "                \"language\": result[\"language\"],\n",
        "                \"latency_ms\": round(latency_ms, 2),\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"text\": \"\",\n",
        "                \"language\": None,\n",
        "                \"latency_ms\": round((time.time() - start_time) * 1000, 2),\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return {\n",
        "            \"module\": \"ASR\",\n",
        "            \"model\": f\"Whisper-{self.model_size}\",\n",
        "            \"target_wer\": \"< 20%\"\n",
        "        }\n",
        "\n",
        "# Initialize\n",
        "asr_module = ASRModule(model_size=\"base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQOF4zPaFm_"
      },
      "source": [
        "---\n",
        "# Module 2: Neural Machine Translation\n",
        "\n",
        "**Model**: IndicTrans2 (ai4bharat/indictrans2-en-indic-dist-200M)  \n",
        "**Input**: English text  \n",
        "**Output**: Hindi text (Devanagari script)  \n",
        "**Success Criteria**: Semantic meaning preserved\n",
        "\n",
        "**Important**: IndicTrans2 requires IndicTransToolkit for preprocessing. Without it, the model will throw \"Invalid source language tag\" errors.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJYGcccoaFm_"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Translation Module\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from IndicTransToolkit.processor import IndicProcessor\n",
        "import time\n",
        "\n",
        "class TranslationModule:\n",
        "    \"\"\"\n",
        "    Neural Machine Translation Module\n",
        "    Converts English text to Hindi using IndicTrans2.\n",
        "\n",
        "    Technical Metric: Semantic accuracy\n",
        "    Target: Meaning preserved accurately\n",
        "\n",
        "    Note: Uses IndicProcessor for required preprocessing/postprocessing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hf_token=None):\n",
        "        self.model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.src_lang = \"eng_Latn\"\n",
        "        self.tgt_lang = \"hin_Deva\"\n",
        "\n",
        "        print(f\"Loading IndicTrans2 model on {self.device}...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            trust_remote_code=True,\n",
        "            token=hf_token\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            trust_remote_code=True,\n",
        "            token=hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        ).to(self.device)\n",
        "\n",
        "        # IndicProcessor handles required preprocessing\n",
        "        self.processor = IndicProcessor(inference=True)\n",
        "\n",
        "        print(f\"Translation Module initialized (IndicTrans2-200M)\")\n",
        "\n",
        "    def translate(self, text, src_lang=\"eng_Latn\", tgt_lang=\"hin_Deva\"):\n",
        "        \"\"\"\n",
        "        Translate text from English to Hindi.\n",
        "\n",
        "        Args:\n",
        "            text: English text to translate\n",
        "            src_lang: Source language code (eng_Latn for English)\n",
        "            tgt_lang: Target language code (hin_Deva for Hindi)\n",
        "\n",
        "        Returns:\n",
        "            dict with keys: success, source_text, translated_text, latency_ms, error\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Step 1: Preprocess with IndicProcessor (required)\n",
        "            input_sentences = [text]\n",
        "            batch = self.processor.preprocess_batch(\n",
        "                input_sentences,\n",
        "                src_lang=src_lang,\n",
        "                tgt_lang=tgt_lang\n",
        "            )\n",
        "\n",
        "            # Step 2: Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                batch,\n",
        "                truncation=True,\n",
        "                padding=\"longest\",\n",
        "                return_tensors=\"pt\",\n",
        "                return_attention_mask=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Step 3: Generate translation\n",
        "            with torch.no_grad():\n",
        "                generated_tokens = self.model.generate(\n",
        "                    **inputs,\n",
        "                    use_cache=True,\n",
        "                    min_length=0,\n",
        "                    max_length=256,\n",
        "                    num_beams=5,\n",
        "                    num_return_sequences=1\n",
        "                )\n",
        "\n",
        "            # Step 4: Decode tokens\n",
        "            decoded = self.tokenizer.batch_decode(\n",
        "                generated_tokens,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            # Step 5: Postprocess with IndicProcessor (required)\n",
        "            translations = self.processor.postprocess_batch(decoded, lang=tgt_lang)\n",
        "\n",
        "            translated_text = translations[0]\n",
        "            latency_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"source_text\": text,\n",
        "                \"translated_text\": translated_text,\n",
        "                \"src_lang\": src_lang,\n",
        "                \"tgt_lang\": tgt_lang,\n",
        "                \"latency_ms\": round(latency_ms, 2),\n",
        "                \"error\": None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"source_text\": text,\n",
        "                \"translated_text\": \"\",\n",
        "                \"latency_ms\": round((time.time() - start_time) * 1000, 2),\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return {\n",
        "            \"module\": \"Translation\",\n",
        "            \"model\": \"IndicTrans2-200M\",\n",
        "            \"target\": \"Semantic accuracy\"\n",
        "        }\n",
        "\n",
        "# Initialize\n",
        "translation_module = TranslationModule(hf_token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFi8pDEaFnA"
      },
      "source": [
        "---\n",
        "# Module 3: Text-to-Speech Synthesis\n",
        "\n",
        "**Model**: Chatterbox Multilingual (ResembleAI, 0.5B parameters)  \n",
        "**Input**: Hindi text (Devanagari script)  \n",
        "**Output**: Hindi audio waveform  \n",
        "**Success Criteria**: Clear, intelligible pronunciation\n",
        "\n",
        "**Note**: Chatterbox supports 23 languages including Hindi. The multilingual model must be installed from GitHub (not PyPI) for Hindi support.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0nHgxMFaFnA"
      },
      "outputs": [],
      "source": [
        "# Cell 7: TTS Module\n",
        "\n",
        "import torchaudio as ta\n",
        "import torch\n",
        "import time\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Fix for perth watermarker compatibility issue\n",
        "class MockWatermarker:\n",
        "    \"\"\"Mock watermarker for environments where perth version is incompatible.\"\"\"\n",
        "    def apply_watermark(self, wav, sample_rate):\n",
        "        return wav\n",
        "\n",
        "import perth\n",
        "if not hasattr(perth, 'PerthImplicitWatermarker'):\n",
        "    perth.PerthImplicitWatermarker = MockWatermarker\n",
        "    print(\"Patched perth.PerthImplicitWatermarker for compatibility\")\n",
        "\n",
        "class TTSModule:\n",
        "    \"\"\"\n",
        "    Text-to-Speech Synthesis Module\n",
        "    Converts Hindi text to Hindi audio using Chatterbox Multilingual.\n",
        "\n",
        "    Technical Metric: Intelligibility, naturalness\n",
        "    Target: Clear pronunciation, natural prosody\n",
        "\n",
        "    Supported languages: 23 including Hindi (hi), English (en), and others.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Loading Chatterbox Multilingual on {self.device}...\")\n",
        "\n",
        "        try:\n",
        "            from chatterbox.mtl_tts import ChatterboxMultilingualTTS\n",
        "            self.model = ChatterboxMultilingualTTS.from_pretrained(device=self.device)\n",
        "            self.sample_rate = self.model.sr\n",
        "            self.is_multilingual = True\n",
        "            print(f\"TTS Module initialized (Multilingual - 23 languages)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Multilingual model failed: {e}\")\n",
        "            print(\"Attempting English-only fallback...\")\n",
        "            from chatterbox.tts import ChatterboxTTS\n",
        "            self.model = ChatterboxTTS.from_pretrained(device=self.device)\n",
        "            self.sample_rate = self.model.sr\n",
        "            self.is_multilingual = False\n",
        "            print(f\"TTS Module initialized (English-only)\")\n",
        "\n",
        "    def synthesize(self, text, language=\"hi\", exaggeration=0.5, cfg_weight=0.5):\n",
        "        \"\"\"\n",
        "        Synthesize speech from text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to synthesize\n",
        "            language: Language code ('hi' for Hindi, 'en' for English)\n",
        "            exaggeration: Emotion intensity (0.0-1.0, default 0.5)\n",
        "            cfg_weight: Classifier-free guidance weight (0.0-1.0, default 0.5)\n",
        "\n",
        "        Returns:\n",
        "            dict with keys: success, audio, sample_rate, latency_ms, error\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if self.is_multilingual:\n",
        "                wav = self.model.generate(\n",
        "                    text,\n",
        "                    language_id=language,\n",
        "                    exaggeration=exaggeration,\n",
        "                    cfg_weight=cfg_weight\n",
        "                )\n",
        "            else:\n",
        "                wav = self.model.generate(\n",
        "                    text,\n",
        "                    exaggeration=exaggeration,\n",
        "                    cfg_weight=cfg_weight\n",
        "                )\n",
        "\n",
        "            latency_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"audio\": wav,\n",
        "                \"sample_rate\": self.sample_rate,\n",
        "                \"latency_ms\": round(latency_ms, 2),\n",
        "                \"language\": language,\n",
        "                \"error\": None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"audio\": None,\n",
        "                \"sample_rate\": self.sample_rate,\n",
        "                \"latency_ms\": round((time.time() - start_time) * 1000, 2),\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def save_audio(self, wav, output_path):\n",
        "        \"\"\"Save audio tensor to file.\"\"\"\n",
        "        ta.save(output_path, wav, self.sample_rate)\n",
        "        return output_path\n",
        "\n",
        "    def play_audio(self, wav):\n",
        "        \"\"\"Play audio in notebook.\"\"\"\n",
        "        display(Audio(wav.squeeze().cpu().numpy(), rate=self.sample_rate))\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return {\n",
        "            \"module\": \"TTS\",\n",
        "            \"model\": \"Chatterbox-Multilingual-0.5B\",\n",
        "            \"multilingual\": self.is_multilingual,\n",
        "            \"target\": \"Clear intelligibility\"\n",
        "        }\n",
        "\n",
        "# Initialize\n",
        "tts_module = TTSModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t3DmbZ_aFnB"
      },
      "source": [
        "---\n",
        "# Central Orchestrator\n",
        "\n",
        "**Role**: Coordinates all modules, validates outputs, handles errors\n",
        "\n",
        "**Design Principle**:The Orchestrator controls flow based on deterministic rules. Each module performs its specific task without awareness of the overall pipeline.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7IC3oI96aFnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6325b08c-d3c8-40d0-a88b-99c07a6dced1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Box initialized\n",
            "\n",
            "============================================================\n",
            "TALKSYNC ARCHITECTURE\n",
            "============================================================\n",
            "Module 1 (ASR):         Whisper-base\n",
            "Module 2 (Translation): IndicTrans2-200M\n",
            "Module 3 (TTS):         Chatterbox-Multilingual-0.5B\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Decision Box\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Any\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class PipelineResult:\n",
        "    \"\"\"Container for pipeline execution results.\"\"\"\n",
        "    success: bool\n",
        "    english_text: str\n",
        "    hindi_text: str\n",
        "    audio: Optional[Any]\n",
        "    total_latency_ms: float\n",
        "    module_latencies: dict\n",
        "    errors: list\n",
        "\n",
        "class DecisionBox:\n",
        "    \"\"\"\n",
        "    Central Orchestrator for the TalkSync Pipeline.\n",
        "\n",
        "    Responsibilities:\n",
        "    1. Route data through pipeline sequentially (ASR -> Translation -> TTS)\n",
        "    2. Validate each module output before proceeding\n",
        "    3. Handle errors gracefully with meaningful messages\n",
        "    4. Log metrics for performance monitoring\n",
        "\n",
        "    Design Principle:\n",
        "    The Decision Box makes routing decisions based on module outputs.\n",
        "    Each module is a separate concern with its own success criteria.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, asr, translation, tts):\n",
        "        self.asr = asr\n",
        "        self.translation = translation\n",
        "        self.tts = tts\n",
        "\n",
        "        # Thresholds for validation\n",
        "        self.max_latency_ms = 10000  # 10 seconds max per module\n",
        "        self.min_text_length = 2     # Minimum valid output\n",
        "\n",
        "        print(\"Decision Box initialized\")\n",
        "        self._print_architecture()\n",
        "\n",
        "    def _print_architecture(self):\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TALKSYNC ARCHITECTURE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Module 1 (ASR):         {self.asr.get_metrics()['model']}\")\n",
        "        print(f\"Module 2 (Translation): {self.translation.get_metrics()['model']}\")\n",
        "        print(f\"Module 3 (TTS):         {self.tts.get_metrics()['model']}\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    def process_audio(self, audio_path, verbose=True):\n",
        "        \"\"\"\n",
        "        Full pipeline: Audio -> Text -> Translation -> Speech\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to English audio file\n",
        "            verbose: Print progress updates\n",
        "\n",
        "        Returns:\n",
        "            PipelineResult with all outputs and metrics\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        errors = []\n",
        "        latencies = {}\n",
        "\n",
        "        # MODULE 1: ASR\n",
        "        if verbose:\n",
        "            print(\"\\n[1/3] ASR: Transcribing English audio...\")\n",
        "\n",
        "        asr_result = self.asr.transcribe(audio_path)\n",
        "        latencies[\"asr\"] = asr_result[\"latency_ms\"]\n",
        "\n",
        "        if not asr_result[\"success\"]:\n",
        "            errors.append(f\"ASR failed: {asr_result['error']}\")\n",
        "            return PipelineResult(\n",
        "                success=False, english_text=\"\", hindi_text=\"\", audio=None,\n",
        "                total_latency_ms=(time.time() - start_time) * 1000,\n",
        "                module_latencies=latencies, errors=errors\n",
        "            )\n",
        "\n",
        "        english_text = asr_result[\"text\"]\n",
        "        if verbose:\n",
        "            print(f\"    Transcribed: \\\"{english_text}\\\"\")\n",
        "            print(f\"    Latency: {asr_result['latency_ms']:.0f}ms\")\n",
        "\n",
        "        # MODULE 2: TRANSLATION\n",
        "        if verbose:\n",
        "            print(\"\\n[2/3] Translation: English -> Hindi...\")\n",
        "\n",
        "        trans_result = self.translation.translate(english_text)\n",
        "        latencies[\"translation\"] = trans_result[\"latency_ms\"]\n",
        "\n",
        "        if not trans_result[\"success\"]:\n",
        "            errors.append(f\"Translation failed: {trans_result['error']}\")\n",
        "            return PipelineResult(\n",
        "                success=False, english_text=english_text, hindi_text=\"\", audio=None,\n",
        "                total_latency_ms=(time.time() - start_time) * 1000,\n",
        "                module_latencies=latencies, errors=errors\n",
        "            )\n",
        "\n",
        "        hindi_text = trans_result[\"translated_text\"]\n",
        "        if verbose:\n",
        "            print(f\"    Translated: \\\"{hindi_text}\\\"\")\n",
        "            print(f\"    Latency: {trans_result['latency_ms']:.0f}ms\")\n",
        "\n",
        "        # MODULE 3: TTS\n",
        "        if verbose:\n",
        "            print(\"\\n[3/3] TTS: Generating Hindi speech...\")\n",
        "\n",
        "        tts_result = self.tts.synthesize(hindi_text, language=\"hi\")\n",
        "        latencies[\"tts\"] = tts_result[\"latency_ms\"]\n",
        "\n",
        "        if not tts_result[\"success\"]:\n",
        "            errors.append(f\"TTS failed: {tts_result['error']}\")\n",
        "            return PipelineResult(\n",
        "                success=False, english_text=english_text, hindi_text=hindi_text, audio=None,\n",
        "                total_latency_ms=(time.time() - start_time) * 1000,\n",
        "                module_latencies=latencies, errors=errors\n",
        "            )\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Audio generated\")\n",
        "            print(f\"    Latency: {tts_result['latency_ms']:.0f}ms\")\n",
        "\n",
        "        total_latency = (time.time() - start_time) * 1000\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"PIPELINE COMPLETE\")\n",
        "            print(f\"Total Latency: {total_latency:.0f}ms\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "        return PipelineResult(\n",
        "            success=True,\n",
        "            english_text=english_text,\n",
        "            hindi_text=hindi_text,\n",
        "            audio=tts_result[\"audio\"],\n",
        "            total_latency_ms=total_latency,\n",
        "            module_latencies=latencies,\n",
        "            errors=[]\n",
        "        )\n",
        "\n",
        "    def process_text(self, english_text, verbose=True):\n",
        "        \"\"\"\n",
        "        Text-only pipeline: Skip ASR, start from English text.\n",
        "        Useful for testing Translation + TTS modules.\n",
        "\n",
        "        Args:\n",
        "            english_text: English text to translate\n",
        "            verbose: Print progress updates\n",
        "\n",
        "        Returns:\n",
        "            PipelineResult with all outputs and metrics\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        latencies = {\"asr\": 0}\n",
        "        errors = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nInput: \\\"{english_text}\\\"\")\n",
        "            print(\"\\n[1/2] Translation: English -> Hindi...\")\n",
        "\n",
        "        trans_result = self.translation.translate(english_text)\n",
        "        latencies[\"translation\"] = trans_result[\"latency_ms\"]\n",
        "\n",
        "        if not trans_result[\"success\"]:\n",
        "            errors.append(f\"Translation failed: {trans_result['error']}\")\n",
        "            return PipelineResult(\n",
        "                success=False, english_text=english_text, hindi_text=\"\", audio=None,\n",
        "                total_latency_ms=(time.time() - start_time) * 1000,\n",
        "                module_latencies=latencies, errors=errors\n",
        "            )\n",
        "\n",
        "        hindi_text = trans_result[\"translated_text\"]\n",
        "        if verbose:\n",
        "            print(f\"    Translated: \\\"{hindi_text}\\\"\")\n",
        "            print(\"\\n[2/2] TTS: Generating Hindi speech...\")\n",
        "\n",
        "        tts_result = self.tts.synthesize(hindi_text, language=\"hi\")\n",
        "        latencies[\"tts\"] = tts_result[\"latency_ms\"]\n",
        "\n",
        "        if not tts_result[\"success\"]:\n",
        "            errors.append(f\"TTS failed: {tts_result['error']}\")\n",
        "            return PipelineResult(\n",
        "                success=False, english_text=english_text, hindi_text=hindi_text, audio=None,\n",
        "                total_latency_ms=(time.time() - start_time) * 1000,\n",
        "                module_latencies=latencies, errors=errors\n",
        "            )\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Audio generated\")\n",
        "\n",
        "        total_latency = (time.time() - start_time) * 1000\n",
        "        if verbose:\n",
        "            print(f\"\\nTotal Latency: {total_latency:.0f}ms\")\n",
        "\n",
        "        return PipelineResult(\n",
        "            success=True,\n",
        "            english_text=english_text,\n",
        "            hindi_text=hindi_text,\n",
        "            audio=tts_result[\"audio\"],\n",
        "            total_latency_ms=total_latency,\n",
        "            module_latencies=latencies,\n",
        "            errors=[]\n",
        "        )\n",
        "\n",
        "# Initialize Decision Box\n",
        "decision_box = DecisionBox(\n",
        "    asr=asr_module,\n",
        "    translation=translation_module,\n",
        "    tts=tts_module\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu8ygMXwaFnD"
      },
      "source": [
        "---\n",
        "# Interactive Demo Interface (Gradio)\n",
        "\n",
        "Provides user-friendly interface for:\n",
        "1. Recording audio from microphone\n",
        "2. Uploading audio files\n",
        "3. Text input for translation\n",
        "4. Playing generated Hindi audio\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Gradio Interface with Clear Metrics\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def format_metrics(result, include_asr=True):\n",
        "    \"\"\"\n",
        "    Format business and technical metrics for display.\n",
        "    \"\"\"\n",
        "    latency_target = 5000  # 5 seconds\n",
        "\n",
        "    metrics_text = f\"\"\"\n",
        "======================================================================\n",
        "                    TALKSYNC METRICS DASHBOARD\n",
        "======================================================================\n",
        "\n",
        "BUSINESS METRICS\n",
        "----------------------------------------------------------------------\n",
        "  End-to-End Latency:     {result.total_latency_ms:.0f}ms (target: under 5000ms)\n",
        "                          {\"Within target - acceptable for demo\" if result.total_latency_ms < latency_target else \"Above target - user will notice delay\"}\n",
        "\n",
        "  Translation Completed:  {\"Yes - Hindi text generated\" if result.hindi_text else \"No - translation failed\"}\n",
        "\n",
        "  Audio Generated:        {\"Yes - Hindi speech ready to play\" if result.audio is not None else \"No - TTS failed to produce audio\"}\n",
        "\n",
        "TECHNICAL METRICS\n",
        "----------------------------------------------------------------------\"\"\"\n",
        "\n",
        "    # Per-module breakdown\n",
        "    for module, latency in result.module_latencies.items():\n",
        "        if latency > 0:\n",
        "            # Add context for each module\n",
        "            if module == \"asr\":\n",
        "                context = \"Time to convert speech to text\"\n",
        "            elif module == \"translation\":\n",
        "                context = \"Time to translate English to Hindi\"\n",
        "            elif module == \"tts\":\n",
        "                context = \"Time to generate Hindi audio\"\n",
        "            else:\n",
        "                context = \"\"\n",
        "\n",
        "            metrics_text += f\"\\n  {module.upper():12} {latency:>6.0f}ms    ({context})\"\n",
        "\n",
        "    # Latency breakdown as percentage\n",
        "    total = result.total_latency_ms\n",
        "    if total > 0:\n",
        "        metrics_text += \"\\n\\n  Latency Breakdown:\"\n",
        "        for module, latency in result.module_latencies.items():\n",
        "            if latency > 0:\n",
        "                percentage = (latency / total) * 100\n",
        "                bar_length = int(percentage / 5)  # Scale to max 20 chars\n",
        "                bar = \"█\" * bar_length\n",
        "                metrics_text += f\"\\n    {module.upper():12} {bar} {percentage:.0f}%\"\n",
        "\n",
        "    # Architecture\n",
        "    metrics_text += f\"\"\"\n",
        "\n",
        "ARCHITECTURE\n",
        "----------------------------------------------------------------------\n",
        "  Module 1 (ASR):         {asr_module.get_metrics()['model']}\n",
        "  Module 2 (Translation): {translation_module.get_metrics()['model']}\n",
        "  Module 3 (TTS):         {tts_module.get_metrics()['model']}\"\"\"\n",
        "\n",
        "    # Errors if any\n",
        "    if result.errors:\n",
        "        metrics_text += \"\\n\\nERRORS\"\n",
        "        metrics_text += \"\\n----------------------------------------------------------------------\"\n",
        "        for error in result.errors:\n",
        "            error_short = error[:80] + \"...\" if len(error) > 80 else error\n",
        "            metrics_text += f\"\\n  {error_short}\"\n",
        "\n",
        "    # Recommendation\n",
        "    if result.total_latency_ms > latency_target:\n",
        "        # Find the slowest module\n",
        "        slowest_module = max(result.module_latencies.items(), key=lambda x: x[1])\n",
        "        metrics_text += f\"\"\"\n",
        "\n",
        "RECOMMENDATION\n",
        "----------------------------------------------------------------------\n",
        "  Bottleneck: {slowest_module[0].upper()} module is taking {slowest_module[1]:.0f}ms\n",
        "  Suggestion: Use shorter sentences to reduce processing time\"\"\"\n",
        "\n",
        "    metrics_text += \"\\n\\n======================================================================\"\n",
        "\n",
        "    return metrics_text\n",
        "\n",
        "\n",
        "def process_audio_input(audio):\n",
        "    \"\"\"\n",
        "    Process audio input from microphone or file upload.\n",
        "    \"\"\"\n",
        "    if audio is None:\n",
        "        return \"No audio provided\", \"\", None, \"Error: No audio input\"\n",
        "\n",
        "    try:\n",
        "        sample_rate, audio_data = audio\n",
        "\n",
        "        if audio_data.dtype == np.int16:\n",
        "            audio_data = audio_data.astype(np.float32) / 32768.0\n",
        "        elif audio_data.dtype == np.int32:\n",
        "            audio_data = audio_data.astype(np.float32) / 2147483648.0\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
        "            temp_path = f.name\n",
        "            import soundfile as sf\n",
        "            sf.write(temp_path, audio_data, sample_rate)\n",
        "\n",
        "        result = decision_box.process_audio(temp_path, verbose=False)\n",
        "        os.unlink(temp_path)\n",
        "\n",
        "        if result.success:\n",
        "            hindi_audio = result.audio.squeeze().cpu().numpy()\n",
        "            metrics_text = format_metrics(result, include_asr=True)\n",
        "            return result.english_text, result.hindi_text, (tts_module.sample_rate, hindi_audio), metrics_text\n",
        "        else:\n",
        "            metrics_text = format_metrics(result, include_asr=True)\n",
        "            return result.english_text or \"Transcription failed\", result.hindi_text or \"\", None, metrics_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"\", None, f\"Exception: {str(e)}\"\n",
        "\n",
        "\n",
        "def process_text_input(english_text):\n",
        "    \"\"\"\n",
        "    Process text input (skip ASR).\n",
        "    \"\"\"\n",
        "    if not english_text or not english_text.strip():\n",
        "        return \"\", None, \"Error: No text provided\"\n",
        "\n",
        "    try:\n",
        "        result = decision_box.process_text(english_text.strip(), verbose=False)\n",
        "\n",
        "        if result.success:\n",
        "            hindi_audio = result.audio.squeeze().cpu().numpy()\n",
        "            metrics_text = format_metrics(result, include_asr=False)\n",
        "            return result.hindi_text, (tts_module.sample_rate, hindi_audio), metrics_text\n",
        "        else:\n",
        "            metrics_text = format_metrics(result, include_asr=False)\n",
        "            return result.hindi_text or \"\", None, metrics_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"\", None, f\"Exception: {str(e)}\"\n",
        "\n",
        "\n",
        "# Build Gradio Interface\n",
        "with gr.Blocks(title=\"TalkSync Demo\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # TalkSync - English to Hindi Real-Time Translation\n",
        "\n",
        "    Translate English speech or text to Hindi audio.\n",
        "\n",
        "    **Pipeline**: Audio/Text -> ASR (Whisper) -> Translation (IndicTrans2) -> TTS (Chatterbox)\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Audio Input\"):\n",
        "            gr.Markdown(\"Record from microphone or upload an audio file.\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    audio_input = gr.Audio(\n",
        "                        sources=[\"microphone\", \"upload\"],\n",
        "                        type=\"numpy\",\n",
        "                        label=\"English Audio Input\"\n",
        "                    )\n",
        "                    audio_submit = gr.Button(\"Translate Audio\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    audio_english_out = gr.Textbox(label=\"Transcribed English Text\", lines=2)\n",
        "                    audio_hindi_out = gr.Textbox(label=\"Translated Hindi Text\", lines=2)\n",
        "                    audio_output = gr.Audio(label=\"Hindi Audio Output\", type=\"numpy\")\n",
        "\n",
        "            audio_metrics = gr.Textbox(\n",
        "                label=\"Metrics Dashboard\",\n",
        "                lines=25,\n",
        "                max_lines=30\n",
        "            )\n",
        "\n",
        "            audio_submit.click(\n",
        "                fn=process_audio_input,\n",
        "                inputs=[audio_input],\n",
        "                outputs=[audio_english_out, audio_hindi_out, audio_output, audio_metrics]\n",
        "            )\n",
        "\n",
        "        with gr.TabItem(\"Text Input\"):\n",
        "            gr.Markdown(\"Enter English text directly (skips ASR module).\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"English Text\",\n",
        "                        placeholder=\"Enter English text here...\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    text_submit = gr.Button(\"Translate Text\", variant=\"primary\")\n",
        "\n",
        "                    gr.Markdown(\"**Example sentences:**\")\n",
        "                    gr.Examples(\n",
        "                        examples=[\n",
        "                            [\"Hello, how are you?\"],\n",
        "                            [\"The meeting is at 3 PM.\"],\n",
        "                            [\"Please send the report.\"],\n",
        "                            [\"Thank you for your help.\"]\n",
        "                        ],\n",
        "                        inputs=[text_input]\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    text_hindi_out = gr.Textbox(label=\"Translated Hindi Text\", lines=2)\n",
        "                    text_audio_output = gr.Audio(label=\"Hindi Audio Output\", type=\"numpy\")\n",
        "\n",
        "            text_metrics = gr.Textbox(\n",
        "                label=\"Metrics Dashboard\",\n",
        "                lines=25,\n",
        "                max_lines=30\n",
        "            )\n",
        "\n",
        "            text_submit.click(\n",
        "                fn=process_text_input,\n",
        "                inputs=[text_input],\n",
        "                outputs=[text_hindi_out, text_audio_output, text_metrics]\n",
        "            )\n",
        "\n",
        "print(\"Launching Gradio interface...\")\n",
        "demo.launch(share=True, debug=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "mT4j8f6uCfAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dTLlv0RlDt_W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQDS_ONaFnE"
      },
      "source": [
        "---\n",
        "# Demo Summary\n",
        "\n",
        "## What This Demo Shows\n",
        "\n",
        "1. **Architecture**: Orchestration pattern with three independent modules\n",
        "2. **ASR**: Whisper converts English audio to text\n",
        "3. **Translation**: IndicTrans2 converts English text to Hindi\n",
        "4. **TTS**: Chatterbox synthesizes Hindi audio\n",
        "5. **Metrics**: Business and technical metrics for evaluation\n",
        "\n",
        "## Key Technical Decisions\n",
        "\n",
        "| Decision | Rationale |\n",
        "|----------|----------|\n",
        "| Whisper base model | Balance of speed and accuracy for demo |\n",
        "| IndicTrans2 with IndicProcessor | Required preprocessing for proper translation |\n",
        "| Chatterbox from GitHub | PyPI version lacks multilingual support |\n",
        "| Decision Box pattern | Separation of concerns, easier debugging |\n",
        "| Gradio interface | User-friendly demo without custom frontend |\n",
        "\n",
        "## Phase 2 Roadmap\n",
        "\n",
        "- Video conferencing platform integration (Zoom, Meet, Teams)\n",
        "- Real-time streaming (chunk-based processing)\n",
        "- Bidirectional translation (Hindi to English)\n",
        "- Voice selection and customization\n",
        "- Custom vocabulary/glossary support\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}